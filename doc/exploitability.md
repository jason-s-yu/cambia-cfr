# Exploitability Calculation in Cambia CFR+

This document explains the concept of exploitability as used in this CFR+ training suite for Cambia, how it's currently calculated, the associated performance bottlenecks, and potential future optimizations.

## Definitions

In the context of two-player zero-sum games like head-to-head Cambia, exploitability is a standard measure of how close a learned strategy is to a Nash Equilibrium. It quantifies the maximum expected utility an opponent could gain by playing a BEST RESPONSE (BR) strategy against our agent's current strategy, assuming the opponent has perfect knowledge of our strategy. Our goal is to obtain a Nash Equilibrium strategy which is unexploitable, meaning the best response against it yields zero additional utility for the opponent (ignoring ties). Therefore, the goal of CFR training is typically to minimize exploitability, driving it towards zero. Exploitability, then, is a variable which tracks the strength of our strategy (how hard is it for an opponent to take advantage of it?). Lower exploitability is better.

While CFR variants like CFR+ (using Outcome Sampling Monte Carlo - OSMCFR in our case) aim to converge to a Nash Equilibrium, we periodically calculate the exploitability of the *average strategy* accumulated over all iterations to track progress. The average strategy represents the agent's overall policy learned so far.

## Current Calculation (`src/analysis_tools.py`)

The exploitability calculation is performed within the `AnalysisTools` class in `src/analysis_tools.py`. The process involves four main steps:

1. **Compute Average Strategy.** The trainer first calculates the agent's current average strategy by normalizing the `strategy_sum` dictionary using the `reach_prob_sum` for each infoset (`CFRDataManagerMixin.compute_average_strategy`). This results in a policy where the probability of choosing an action is proportional to how often it has been historically beneficial, weighted by reach probability.
2. **Compute Best Response Value (for each player).** The core of the calculation lies in the `_compute_best_response_value` method. This function calculates the maximum expected utility one player (the "BR player") could achieve if the *other* player adheres strictly to the fixed average strategy computed in step 1. This is done for both players (P0 assuming P1 plays the average strategy, and P1 assuming P0 plays the average strategy).
3. **Best Response Recursion (`_best_response_recursive`).** This private recursive function simulates the game from the start to calculate the BR value:
    * If BR player turn: The function explores *all* legal actions. For each action, it recursively calls itself to determine the value of the resulting game state and chooses the action that yields the maximum possible value. It plays greedily optimal against the opponent's known fixed strategy.
    * If opponent turn: The function retrieves the opponent's average strategy for the current infoset (using an `AgentState` representing the opponent's view). It calculates the EV by averaging the values of the resulting states for each of the opponent's possible actions, weighted by the probabilities assigned in their average strategy.
4. **Final Exploitability.** The overall exploitability is the average of the best response values calculated for Player 0 and Player 1: `exploitability = (BR_Value(P0) + BR_Value(P1)) / 2`.

## Performance Bottlenecks

The current exploitability calculation represents a significant performance bottleneck during training. The BR calculation requires exploring a large portion of the game tree, especially for the BR player who considers all moves at their decision nodes. At the moment, we implement a `_best_response_recursive` function as a standard, single-threaded recursive function. It runs entirely within the main training process *after* worker results for an iteration are merged. Additionally, it runs within the main thread, blocking other main thread responsibilities (logger aggregation, live display, etc.). Initial benchmarks show that the OSMCFR worker simulations complete quickly (within seconds), but the exploitability step can take a very long time (likely hours).

## TODO

1. (A) Launch exploitability in separate thread
    * **Concept:** Move the entire exploitability calculation (computing average strategy + two BR calls) into a separate `threading.Thread`. The main loop spawns this thread and continues training without waiting.
    * **Pros:** Frees the main training loop, improving responsiveness and allowing subsequent iterations to start sooner. Simpler to implement than full parallelization.
    * **Cons:** Does *not* speed up the actual exploitability calculation (which still runs serially within its thread). Subject to Python's GIL, limiting CPU parallelism for the calculation itself. Adds complexity for thread management and result synchronization.

2. (B) Parallelize the BR calculation
    * **Concept:** Modify the `_best_response_recursive` function. When the BR player needs to choose an action, distribute the recursive calls for evaluating each possible action across multiple worker processes using `multiprocessing.Pool`. Collect the results and find the maximum value.
    * **Pros:** Directly addresses the computational bottleneck by leveraging multiple CPU cores for the tree search, significantly speeding up the calculation. Avoids GIL limitations.
    * **Cons:** More complex implementation involving process management, inter-process communication (IPC) for game state/strategy data, and result aggregation. Potential overhead from data serialization (pickling).

3. Merge A+B?
    * **Concept:** Implement Option A (separate thread) and have that thread execute Option B (parallelized BR calculation using processes). An "Exploitability Manager" thread would coordinate the process pool for the BR calculation.
    * **Pros:** Achieves both main thread responsiveness *and* faster exploitability calculation.
    * **Cons:** Maximum complexity, combining thread and process management challenges. Requires careful resource allocation.
